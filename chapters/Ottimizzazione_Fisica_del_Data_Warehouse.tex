\chapter{Ottimizzazione Fisica del Data Warehouse}
L’efficienza e le prestazioni dei sistemi di BI dipendono notevolmente dall’ottimizzazione delle architetture fisiche del \textit{Data Warehouse}. Attraverso tecniche di indicizzazione, partizionamento, compressione e \textit{caching} si esamineranno le scelte relative alla configurazione ed alla gestione delle risorse \textit{hardware} per migliorare la scalabilità, la velocità di accesso e la sostenibilità dell’architettura. Nel contesto dell'elaborato, questo approccio è ritenuto importante come tecnica di supporto alle tecniche di progettazione logica e metodologie di ottimizzazione viste precedentemente.

\section{Tecniche di indicizzazione e partizionamento}
Le tecniche di indicizzazione e partizionamento sono tecniche fondamentali per migliorare le prestazioni e l'accessibilità ai dati in un \textit{data warehouse} e in un sistema di BI caratterizzato da elevate quantità di dati. Per quanto riguarda le tecniche di indicizzazione, rispetto agli indici \textit{RowID}, gli indici \textit{bitmap} permettono di risparmiare in termini di spazio e di tempo per il calcolo. Infatti, con questa tecnica la rappresentazione dei dati è più compatta e si rivela molto utile in ambienti OLAP in cui la cardinalità degli attributi, come i codici prodotto o le categorie, è bassa. Ciò consente di gestire tabelle molto grandi, facilitando le operazioni di ricerca aggregata mediante operazioni \textit{bitwise}. Grazie a questa tecnica è possibile velocizzare le operazioni e ridurre il carico computazionale per effettuare le \textit{query}, offrendo una migliore esperienza di \textit{reporting} multidimensionale (Golfarelli, 2000)\cite{Golfarelli2000}. Ovviamente, l'utilizzo di questa tecnica deve essere attentamente valutato, in quanto, come indicato da Goyal et al. (1999)\cite{GoyalEtAl1999} e Golfarelli (2000)\cite{Golfarelli2000}, se sono frequenti le operazioni di aggiornamento, si riducono i vantaggi che ne derivano dall'uso degli indici \textit{bitmap}.

\input{./adds/Ottimizzazione_Fisica_del_Data_Warehouse/tabella_bitmap}

Il partizionamento dei dati è un'altra tecnica per aumentare le prestazioni del sistema. Tale tecnica permette di partizionare le tabelle di grandi dimensioni in partizioni più piccole per velocizzare le \textit{query} analitiche effettuate su periodi temporali o su segmenti specifici. Alcune possibili tecniche di partizionamento sono quella orizzontale, che partiziona i dati per periodi temporali, e quella verticale, che partiziona i dati tramite gli attributi delle tabelle. Un esempio di tecnica orizzontale è partizionare la tabella dei fatti per trimestre o per anno, per effettuare le analisi storiche più velocemente, riducendo i tempi di scansione dei dati. Con questa tecnica, come indicato da Vaisman e Zimányi (2022)\cite{VaismanZimanyi2022}, si velocizzano le prestazioni e si favoriscono le operazioni di manutenzione, come archiviare o eliminare le partizioni inutilizzate. Anche per questa tecnica è fondamentale effettuare un'analisi preliminare sul sistema, per stabilire l'efficacia che deriva dal partizionamento dei dati, analizzando il sistema, le modalità di accesso ai dati e le esigenze.
\\ \\
La combinazione delle tecniche di partizionamento e indicizzazione può essere un ulteriore approccio per massimizzare i vantaggi di entrambe. Grazie all'utilizzo congiunto di indici \textit{bitmap} su tabelle partizionate è possibile ottenere un accesso simultaneo ai dati storici contenuti nelle partizioni. Ciò si rivela molto importante perché, in molti sistemi di \textit{Business Intelligence}, vengono eseguite \textit{query} su periodi di tempo molto lunghi e tale approccio non va ad intaccare la disponibilità dei dati, così da poter velocizzare le \textit{query}, minimizzando i tempi di risposta. Come indicato da Vaisman e Zimányi (2022)\cite{VaismanZimanyi2022}, occorre anche tenere sotto controllo l'efficacia di queste tecniche con un adeguato monitoraggio, in quanto la \textit{performance} di un \textit{data warehouse} non deve essere compromessa dalle impostazioni precedentemente configurate.

\includesql{./adds/Ottimizzazione_Fisica_del_Data_Warehouse/codice_partizione.sql}{Esempio di creazione di tabella partizionata e vista indicizzata allineata. (Vaisman e Zimányi, 2022 pp.\,282--283)\cite{VaismanZimanyi2022}}{lst:partitioned_view}

Le tecniche di ottimizzazione per i sistemi di BI devono tenere presente che la priorità, rispetto ai sistemi OLTP, deve essere rappresentata dalla riduzione dei tempi di risposta e non dal \textit{throughput}. Ciò comporta l'implementazione di alcune tecniche specifiche di progettazione fisica, quali gli indici \textit{bitmap} e le partizioni sugli attributi maggiormente utilizzati nelle \textit{query}, perché l'obiettivo dei sistemi \textit{Business Intelligence} è effettuare interrogazioni su enormi quantità di dati di anni passati. Quindi, per raggiungere gli obiettivi prefissati, occorre monitorare le prestazioni ed eseguire alcuni \textit{benchmark} per valutare l'efficacia delle tecniche di progettazione fisica introdotte. Come riportato da Golfarelli (2000)\cite{Golfarelli2000} e da Inmon (2002)\cite{Inmon2002}, le tecniche di progettazione fisica più adatte dipendono da come si accede ai dati e in quale modo vengono aggiornati.
\\ \\
Nelle tecniche di progettazione, solitamente, viene spesso trascurato l'effetto che la granularità delle partizioni può avere sulla \textit{performance} del sistema. Partizioni con una granularità molto fine possono comportare un eccessivo \textit{overhead} amministrativo e la frammentazione delle risorse, mentre partizioni troppo ampie rischiano di compromettere la velocità di accesso ai dati. Occorre, quindi, trovare un compromesso in termini di granularità del partizionamento in base ai bisogni del \textit{business}, in modo da ottimizzare la \textit{performance} e la velocità di risposta del sistema, come indicato da Vaisman e Zimányi (2022)\cite{VaismanZimanyi2022}.
\\ \\
Le prospettive future nell'ingegneria computazionale mostrano l'importanza di implementare strategie avanzate di accesso ai dati, dato il continuo incremento in termini di quantità e complessità dei dati nei sistemi di \textit{Business Intelligence}. Negli ultimi anni, come indicato da Khanna et al. (2021)\cite{KhannaEtAl2021}, è cresciuta sempre più l'importanza dell'integrazione tra tecniche di indicizzazione tradizionali e gli algoritmi di \textit{machine learning}. La particolarità di questi ultimi algoritmi è la dinamicità per la gestione degli indici e delle partizioni e la capacità di prevedere il carico di lavoro, aumentando in questo modo la scalabilità e la velocità del sistema.
\\ \\
In aggiunta a questo, il rapidissimo sviluppo delle architetture \textit{Big Data} impone continui aggiornamenti delle tecniche di ottimizzazione, monitoraggio delle prestazioni e dei processi per effettuare un automatico ribilanciamento delle partizioni e degli indici. Da Khanna et al. (2021)\cite{KhannaEtAl2021} viene evidenziato come sia fondamentale adottare una progettazione fisica integrata, in cui le tecniche di ottimizzazione statica (indici \textit{bitmap} e partizionamento) vengono affiancate da tecniche \textit{data-driven}, per migliorare le prestazioni, la scalabilità e la complessità delle \textit{query}.
\\ \\
Un ultimo fattore da non sottovalutare è quello di offrire alle figure professionali esperte una solida base tecnica su cui poter contare. Grazie all'implementazione di tecniche di \textit{machine learning}, ai sistemi di monitoraggio automatizzati, ai nuovi strumenti di progettazione e alla combinazione delle figure professionali esistenti sarà possibile ottenere una migliore esperienza degli utenti, un supporto più affidabile agli operatori, oltre che ai decisori, aumentando di conseguenza la continuità del servizio offerto.

\section{Strategie di compressione e caching}
Le strategie di compressione nei \textit{data warehouse} consentono una sensibile riduzione dello spazio necessario per l’archiviazione, particolarmente critica nei \textit{data warehouse} OLAP. L’utilizzo di algoritmi come LZW permette una riduzione del \SIrange{30}{35}{\percent} rispetto alle dimensioni originarie, raggiungendo un rapporto di compressione del \SIrange{65}{70}{\percent}, secondo Goyal et al. (1999)\cite{GoyalEtAl1999}. Questo consente di risparmiare sull’occupazione di memoria nei contesti di BI, solitamente con elevati volumi di dati e con requisiti di conservazione storica. Tuttavia, è necessario tenere in considerazione l’impatto della compressione/decompressione sulle prestazioni, soprattutto in casi in cui siano necessari processi ripetuti di decompressione. \\

\begin{wrapfigure}{l}{0.50\textwidth}
    \centering
    \includegraphics[width=0.48\textwidth]{./adds/Ottimizzazione_Fisica_del_Data_Warehouse/Huffman_Coding.png}
    \caption{\textit{Huffman Coding}. (Goyal et al., 1999 p. 11-7 )\cite{GoyalEtAl1999}}
    \label{fig:wrap}
\end{wrapfigure}

In realtà, un ulteriore vantaggio consiste nell’incremento delle prestazioni per le \textit{query} che ne beneficiano. Ovvero si ottiene una riduzione delle operazioni di \textit{Input-Output} (I/O) al disco poiché vi è un minore trasferimento di dati dal medesimo. Questa ottimizzazione è particolarmente utile nelle operazioni analitiche tipiche dei \textit{data warehouse}, dove sono richieste \textit{query} con aggregazioni e analisi multidimensionali (Goyal et al., 1999)\cite{GoyalEtAl1999}. Ovviamente, questo incremento di prestazioni è legato al tipo di algoritmo scelto.

\begin{wrapfigure}{r}{0.45\textwidth}
    \centering
    \includegraphics[width=0.43\textwidth]{./adds/Ottimizzazione_Fisica_del_Data_Warehouse/Arithmetic_Coding.png}
    \caption{\textit{Arithmetic Coding}. (Goyal et al., 1999 p. 11-8 )\cite{GoyalEtAl1999}}
    \label{fig:wrap}
\end{wrapfigure}

Ad esempio, l'\textit{Huffman Coding} permette di bilanciare al meglio velocità e capacità di compressione/decompressione, mentre algoritmi più complessi, come l'\textit{Arithmetic Coding}, nonostante migliori prestazioni in termini di percentuale di compressione, si rivelano inadeguati se è richiesta frequente modifica dei dati, tempi di risposta brevi ed elevata affidabilità (Goyal et al., 1999)\cite{GoyalEtAl1999}.
\\ \\
La compressione ha un impatto significativo sulle strategie di \textit{backup}/\textit{recovery} per i \textit{data warehouse}. Riducendo la quantità di dati da gestire, si ha un consistente risparmio di tempo nelle procedure di \textit{backup}, oltre a migliorare lo \textit{storage} secondario per ridurre i costi di archiviazione (Goyal et al., 1999)\cite{GoyalEtAl1999}. L’unico compromesso è rappresentato dalla lentezza nell’accesso ai dati compressi durante il \textit{recovery}, se si considera un ripristino del sistema in un tempo relativamente breve.
\\ \\
Il \textit{caching} permette di rispondere più velocemente alle \textit{query} nel contesto dei sistemi di BI. Questa tecnica consiste nella conservazione in memoria \textit{cache} dei dati più frequentemente richiesti o del risultato di viste materializzate (Cerquitelli \& Garza, 2023)\cite{CerquitelliGarza2023}. L’obiettivo primario è ridurre la latenza delle \textit{query} ripetitive di \textit{reporting} o delle operazioni di analisi interattive, mediante l’identificazione dei dati utili per la \textit{cache}.
\\ \\
Una gestione dinamica del \textit{caching} consente di ridurre drasticamente le operazioni di I/O al disco e di fornire dei risultati aggiornati (Cerquitelli \& Garza, 2023)\cite{CerquitelliGarza2023}. Questo tipo di tecnica implementata dinamicamente è utilizzata anche in sistemi di grandi dimensioni al fine di evitare le limitazioni apportate dall'I/O.
\\ \\
La progettazione fisica del \textit{data warehouse} ha un impatto significativo sulle strategie di compressione e \textit{caching}: condiziona la manutenzione delle viste materializzate e permette di selezionare al meglio le configurazioni per gli indici. L’utilizzo di algoritmi come A* (Labio et al., 1996)\cite{LabioQuassAdelberg1996} permette di analizzare contemporaneamente il costo di memorizzazione, il costo di accesso e l’impatto degli aggiornamenti al \textit{data warehouse}, scartando le configurazioni subottimali e migliorando l’efficienza.

\includepseudo{./adds/Ottimizzazione_Fisica_del_Data_Warehouse/codice_A.tex}{Algoritmo A* per selezionare viste e indici (Labio et al., 1996 p. 10)\cite{LabioQuassAdelberg1996}}{lst:codice_A}

Tramite algoritmi e sistemi di \textit{machine learning} si può determinare in modo dinamico il compromesso tra prestazioni e costo di mantenimento dei sistemi di \textit{data warehouse} basati su viste materializzate (Bhaja et al., 2023)\cite{BhajaEtAl2023}. Questo permette l’adozione delle viste materializzate anche in \textit{data warehouse} complessi con carichi di lavoro variabili.
\\ \\
L’applicazione contemporanea di tecniche di compressione e \textit{caching} sta diventando la più comune in caso di elevati carichi di lavoro e per sistemi di \textit{Business Intelligence} che richiedono scalabilità. La flessibilità, intelligenza e scalabilità di sistemi implementati tramite queste tecniche hanno permesso un ulteriore evoluzione dei moderni \textit{data warehouse}, consentendo di rispondere a carichi di lavoro con variabili caratteristiche (Bhaja et al., 2023)\cite{BhajaEtAl2023}.
\\ \\
Le strategie di compressione e \textit{caching} devono essere parte di un \textit{framework} di ottimizzazione fisica del \textit{data warehouse}. Questo permette di effettuare in tempi ragionevoli le \textit{query} su insiemi di dati di enorme grandezza, pur controllando i tempi di manutenzione delle risorse. Nella letteratura si evidenzia come le prestazioni dei \textit{data warehouse} beneficino da una combinazione di dati compressi e \textit{caching} (Inmon, 2002)\cite{Inmon2002}.
\\ \\
In conclusione, l’applicazione combinata di strategie di compressione e \textit{caching}, con un approccio dinamico e basato su sistemi di progettazione avanzata, è la chiave del miglioramento delle \textit{performance}, mantenendo un’elevata sostenibilità operativa per i \textit{data warehouse} di nuova generazione.

\begingroup
  \renewcommand\figurename{Formule}%
  \begin{figure}[ht]
    \centering
    \input{./adds/Ottimizzazione_Fisica_del_Data_Warehouse/formule_A.tex}
    \caption{Considerazioni per l'algoritmo A*. (Labio et al., 1996 pp. 8-10)\cite{LabioQuassAdelberg1996}}
    \label{fig:formuleA}
  \end{figure}
\endgroup
